1. 강화학습이란? (Reinforcement Learning)
- UnSupervised: No labels, No feedback, Find hidden structure
- Supervised : Labeled data, Direct feedback, predict outcome/future 
- Reinforcement : Decision process, Reward system, Learn seris of actions
- 조작적 조건 형성 : 스키너의 상자
                   배고픈 상태의 흰쥐를 스키너 상자에 넣는다
                   배고픈 상태로 만드는것= 박탈
                   흰쥐는 스키너 상자 안에서 돌아다니다가 우연히 지렛대를 누르게 도니다
                   지렛대를 누르자 먹이가 나온다.
                   지렛대와 먹이 간의 상관관계를 알지 못하는 쥐는 다시 상자 안을 돌아다닌다.
                   다시 우연히 지렛대를 누른 흰 쥐는 또 먹이가 나오는 것을 보고 지렛대를 누르는 행동을 자주 하게 된다.
                   이러한 과정이 반복되면서 흰 쥐는 지렛대를 누르면 먹이가 나온다는 사실을 학습하게된다.
- 기계학습의 한 영역이다. 행동심리학에서 영감을 받았으며, 어떤 환경 안에서 정의된 에이전트가 현재의 상태를
  를 인식하여 선택 가능한 행동들 중 보상을 최대화하는 행동 혹은 행동 순서를 선택하는 방법이다(Wikipidia)
- 잘한 행동에 대해 칭찬 받고 잘못한 행동에 대해 벌을 받은 경험을 통해 자신의 지식을 키워나가는 학습법이다.
 (KAIST 김종환 교수)
- Traial and Error 시도 해보고 배우는 학습, Delayed Reward 보상이 지연될 수 있다.
- Agent(에이전트) : 환경에서 상태를 관찰, 그 상태에대한 어떠한 기준에대한 행동을 선택, 선태한 행동을 환경에서 실행
                    환경으로부터 다음 상태와 보상을 받음, 보상을 통해 에이전트가 가진 정보를 수정(목표지향)
- Environment(환경) : 에이전트를 제외한 나머지 (물리적으로 정의하기 힘듦)
- State(상태) : 현재 상황을 나타내는 정보
- Action(행동) : 현재 상황에서 에이전트가 하는 것
- Reward(보상) : 행동의 좋고 나쁨을 알려주는 정보 
- RL Componets : Environment : 에이전트가 이동하는 세계. 에이전트의 현재 상태 및 행동(입력)을 취하여 
                 보상과 다음 상태(출력)를 반환
                 State & S : 에이전트가 인식하는 구체적이고 즉각적인 자신의 상황이다. 에이전트가
                 마주하는 특정 장소와 시간이며 즉각적인 구성을 의미
                 Agent : 행동을 취하는 주최. 배송 서비스를 수행하는 드론이나, 비디오 게임에서 슈퍼 마리오
                 를 예로 들 수 있다.
                 Action : 에이전트가 취할 수 있는 모든 행동을 말하며 에이전트는 수행 가능한 행동의 리스트
                 중에서 앞으로 할 행동을 선택해야 한다.
                 EX).비디오게임 - 오른쪽이나 왼쪽으로 달리기, 높거나 낮게 점프 (discrete)
                 주식시장 - 유가 증권 및 파생상품을 구매, 판매 또는 보유(discrete)
                 항공 드론 - 3D 공간에서의 여러 가지 속도와 가속도가 될 수 있다. (continues)
                 정책(π) : 에이전트가 현재 상태를 기준으로 다음의 행동을 결정하는 데 사용하는 전략
                 에이전트는 특정한 상태에서 보상을 최대화할 수 있는 행동을 선택
                 Reward(R) : 에이전트의 행동에 대한 성공이나 실패를 측정하는 피드백. 에이전트의 행동
                 에 의해 평가된 보상은 즉시 주어질 수도, 지연될 수 도 있음.
                 할인율(γ감마, Discount factor) : 보통 0과 1사이의 값으로 즉각적으로 주어지는 보상보다
                 상대적으로 갗치가 낮은 미래의 보상을 만들기 위한 고안. γ감마가 0.8이고 3단계를 거쳐 10점의 보상
                 을 받는다면 보상의 현재가치는 0.8³ X 10이다.
                 가치(Value Function): 단기적인 보상인 R과는 달리 Value는 장기적인 관점에서의 현재상태를 할인된
                 모든 보상들의 기대값.vπ(s)란 현재의 상태에서 정책 π에 따른 기대되는 보상을 의미
                 Q-Value Function & Action-Value Functio(Q): Qvalue와 Value는 비슷한데 Q-value는 현재 상태
                 에서 취하는 행동 a를 고려한다는 것이 차이점이다.Qπ(s,a)은 정책 π에 따라 행동 a를 취할 경우
                 현재의 상태 s에서 받을 장기적인 보상을 말한다. Q는 상태- 행동 쌍을 보상에 매핑한다.
- RL Kinds : Model : agent가 관측하는 환경을 modeling 한 것
             Policy based agent : value function 없이 policy와 model 만으로 구성
             Value based agent : policy 없이 value function와 model 만으로 구성
             Model based agent/ Model free agent : model에 대한 정보 = state transition 정보의 유무
             Actor Critic : Policy, Value function, Model 을 모두 사용
             
2. 가치함수, 벨만방정식, MDP
- Value Funtion : 보상(R) : 보상이란 에이전트의 행동에대한 성공이나 실패를 측정하는 피드백. 에이전트의
                  행동에 의해 평가된 보상은 즉시 주어질 수도 지연될 수 도 있음
                  할인율(γ감마, Discount factor) : 보통 0과 1사이의 값으로 즉각적으로 주어지는 보상보다
                  상대적으로 갗치가 낮은 미래의 보상을 만들기 위한 고안. γ감마가 0.8이고 3단계를 거쳐 10점의 보상
                  을 받는다면 보상의 현재가치는 0.8³ X 10이다.
                  Return(G, total discount reward) : 전체 reward를 시간에 따른 감가상각을 포함하여 합산한 것
                  가치(Value Function): expected return starting from state s 현재 상태 s에서부터
                  시작 할 떄 기대되는 return값을 말한다.
- Bellman Equation : 가치함수가 정의 되었을 때 판단 지표 2가지
                     immediate reward Rt+1
                     discounted value of successor state γv(St+1)
- Markov Reward Process : Markov process의 각 state에 reward를 추가하여 확장한 것이다.
			                    S:state 집합을 의미한다. 바둑판에선 바둑돌이 어떻게 놓여져 있는가를, 
			                    미로를 탈출하는 문제에서는 현재 위치를 나타낸다
                          P:각 요소가 p(s'|s)=Pr(St+1=s'|St=s)인 집합이다. p(s'|s)는 현재 상태 s에서 s'으로 이동할 확률
                          을 의미하며, transition probability라고 한다.
                          R:각 요소가 r(s)=E[Rt+1|St=s]인 집합이다 r(s)는 state s에서 얻는 reward를 의미한다.
                          γ:즉각적으로 얻는 reward와 미래의 얻을 수 있는 reward간의 중요도를 조절하는 변수이다.
                          주로 [0, 1]사이의 값을 가지며 discount factor라고 한다.
                          MDP: action이라는 요소를 추가한 모델 <S,A,P,R,γ>라는 tuple로 정의
                          Policy 정책(π):정책은 각 상태 (sΕS)에 대해 Action(aΕA)에 대한 확률 분포를 정의하는 함수
                          state Transition(P): MDP가 주어진 π를 따를 떄, s에서 s'으로 이동할 확률
                          Reward(P): s에서 얻을 수 있는 reward
                          State-value function with policy:MDP에서 v(s)는 Markov reward process의 state-value function과 마찬가
                          지로 state S에서 시작했을 떄 얻을 수 있는 return의 기댓값을 의미한다. 그러나 MDP는 주로
                          어진 policy π를 따라 action을 결정하고, state를 이동한다.
                          Action-value function: qπ(s,a)는 state에서 시작하여 action을 취했을 때 얻을 수 있는 return
                          의 기댓값을 의미한다.
3. Dynamics : Optimal substructure : 작은 문제로 나뉠 수 있어야 함
             Oberlapping subsproblems : 한 서브 문제를 풀고 나온 솔루션을 저장해 다시 사용할수 있음
             MDP는 이 조건을 만조한다.
             Bellman 방정식이 recursive
             value function이 작은 문제들의 해
             Policy Iteration : Initialize π randomly
                                Repeat until converge : Let V= Vπ
                                fpr each state s, let π(s)= arg max aεA γ∑s'εsPsa(s')V*(s')
             Policy Evaluation : Undiscounted episodic MDP (γ=1)
                                 Nonterminal state 1,...,14
                                 One terminal state (Shown twice as shaded squares)
                                 Actions leading out of the grid leave state un changed
                                 Reward is -1 until the terminal statte is reached
                                 Agent follws uniform random policy
- Monte Carlo Method : (0,0),(0,1),(1,0),(1,1)로 이루어진 좌표 상에 임의의 점 (x,y)를 샘플링한다.
                       샘플링한 점이 중심이 (0,0)이고 반지름이 1인 사분원에 속하는 점인지를 판단한다
                       이 과정을 충분히 반복한다.
4. Temporlal Difference : 에피소드 마다 가 아니라 매 타임스텝 마다 가치함수를 업데이트 
                          MC: V(St) <- V(St) +  a(Gt-V(St))
                          TD(0): V(St) <-V(St) + a(Rt+1 γV(St)-V(St))
- N-step TD : Consider the following n-step return for n= 1,2,
- Forward biew TD(λ) : 각스텝에 weight을 다르게 줌 (다 더하면 1)
                       V(St) <- V(St) + a(Gλt- V(St))
                       when λ-return, Gλt = (1- λ)∑λn-1Gt(n)
                                                 n=1
                      MC와 비슷하게 실제적인 업데이트는 Complete episodes를 해야만 가능
- Backward view TD(λ) : Forward view provides theory
                        Backward biew provides mechandism
                        Update online, every step, from incomplete sequences
5. SARSA, Q- Learning
- SARSA(λ): Forward,Backward view TD와 동일함
- Off Policy : Learn from observing humans or ohter agent
               Re-use experience generated from old policies
               Learn about Optimal Policy while follwing exploratory policy
               Learn about multiple policies while follwing one policy
- Q- Learning : 행동하는 정책: ξ-탐욕 정책
                학습하는 정책: 다음상태에서 어떤행동을 할 때 다음 상태의 최대 큐함수를 현재 상태
                의 큐함수로 업데이트
                살사의 큐함수 업데이트 -> 큐러닝의 큐함수 업데이트
                다음상태에서 다음행동을 해보는 것이 아니라 다음 상태에서 가장 큰 큐함수를 가지고 업데이트
                살사의 필요한 샘플 -> 큐러닝의 필요한 샘플
                다음 상태에서 가장 큰 큐함수만 필요하기 떄문에[St,At,Rt+1,St+1] 샘플도 까지만 필요하다.
                현재 state S에서 behavior policy, μ(e.g ξ-greedy)에 따라 action A을 선택
                Q-func을 이용하여 다음 state S에서의 action A는 π(e.g.greedy)에 따라 선택
                Q-learning의 target은 Rt+1 + γQ(St+A') =  Rt+1 + γQ(St+1, argmax Q(St+1,a')
                =  Rt+1 + max γQ(St+1,a')
                Q-func을 업데이트 Q(S,A) <- Q(S,A) + a(R+γmax(S',A')-Q(S,A))
6. Value Function Approximation
- Tabular Method : 테이블로 표기되는 value function들로 예측해왔다. 이것을 테이블로만 표현하기엔 한계가
                   있다. 메모리가 많이들어감
- Function Approximation : 실제로 가지고 있지 않은 data도 func을 통해서 구할 수 있다.
                           실제 data의 noise를 배제하고 traning할 수 있다.
                           고차원의 data도 효율적으로 저장이 가능하다.
